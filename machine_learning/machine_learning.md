# Machine Learning 

## 机器学习简介

### 监督式机器学习

* 机器学习系统通过学习如何组合输入信息来对从未见过的数据做出有用的预测。

### 标签

**标签**是我们要预测的事物，即简单线性回归中的`y`变量。标签可以是小麦未来的价格、图片中显示的动物品种、音频编辑的含义或任何事物。

### 特征

**特征**是输入变量，即简单线性回归中的`x`变量。特征应该是可观察且可量化的指标。简单的机器学习项目可能会使用单个特征，而比较复杂的机器学习项目可能会使用数百万个特征，按如下方式指定：

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{x1,x2, ... xn}

在垃圾邮件检测器示例中，特征可能包括：

* 电子邮件文本中的字词
* 发件人地址
* 发送电子邮件的时段
* 电子邮件中包含“一种奇怪的把戏”这样的短语。

### 样本

**样本**是指数据的特征实例：**x**。（我们采用粗体  **x** 表示它是一个矢量。）我们将样本分为以下两类：

* 有标签样本
* 无标签样本

**有标签样本**同时包含特征和标签。即：

```
  labeled examples: {features, label}: (x, y)
```

我们使用有标签样本来训练模型。在我们的垃圾邮件检测器实例中，有标签样本是用户明确标记为“垃圾邮件”或“非垃圾邮件”的各个电子邮件。

例如，下表显示了从包含加利福尼亚州房价信息的[数据集](https://developers.google.cn/machine-learning/crash-course/california-housing-data-description)中抽取的5个有标签样本：

![label_machine](./images/label_machine.png)

**无样本标签**包含特征，但不包含标签。即：

```
  unlabeled examples: {features, ?}: (x, ?)
```

在使用有标签样本训练了我们的模型之后，我们会使用该模型来预测无标签样本的标签。在垃圾邮件检测器示例中，无标签样本是用户尚未添加标签的新电子邮件。

### 模型

**模型**定义了特征与标签之间的关系。例如，垃圾邮件检测模型可能会将某些特征与“垃圾邮件”紧密联系起来。模型生命周期的两个阶段：

* **训练**表示创建或**学习**模型。也就是说，像模型展示有标签样本，让模型逐渐学习特征与标签之间的关系。
* **推断**表示将训练后的模型应用于无标签样本。也就是说，使用训练后的模型来做出有用的预测（y'）。例如，在推断期间，可以针对新的无标签样本预测`medianHouseValue`。

### 回归与分类

**回归**模型可预测连续值。例如，回归模型做出的预测可回答如下问题：

* 加利福尼亚州一栋房产的价值是多少？
* 用户点击此广告的概率是多少？

**分类**模型可预测离散值。例如，分类模型做出的预测可回答如下问题：

* 某个指定的邮件是垃圾邮件还是非垃圾邮件？
* 这是一张狗、猫还是老鼠的图片？


## 深入了解机器学习

### 线性回归

人们早就知晓，相比凉爽的天气，蟋蟀在较为炎热的天气里叫的更为频繁。数十年来，专业和业余昆虫学者已将每分钟的鸣叫声和温度方面的数据编入目录。Ruth 阿姨将她喜爱的蟋蟀数据库作为生日礼物送给您，并邀请您自己利用该数据库训练一个模型，从而预测鸣叫声与温度的关系。

首先建议您将数据绘制成图表，了解下数据的分布情况：

![2.png](./images/2.png)

**图 1. 每分钟的鸣叫声与温度（摄氏度）的关系。**

毫无疑问，此曲线图表明温度随着鸣叫声次数的增加而上升。鸣叫声与温度之间的关系是线性关系吗？是的，您可以绘制一条直线来近似地表示这种关系，如下所示：

![3.png](./images/3.png)

**图 2. 线性关系。**

事实上，虽然该直线并未精确无误地经过每个点，但针对我们拥有的数据，清楚地显示了鸣叫声与温度之间的关系。只需运用一点代数知识，您就可以将这种关系写下来，如下所示：

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;y = mx + b

其中：

* y 指的是温度（以摄氏度表示），即我们试图预测的值。
* m 指的是直线的斜率。
* x 指的是每分钟鸣叫声次数，即输入特征的值。
* b 指的是y 轴截距。

按照机器学习的惯例，需要写一个存在细微差别的模型方程式：

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;y' = b + w1x1

其中：

* y' 指的是预测标签（理想输出值）。
* b 指的是偏差（y 轴截距）。
* w1 指的是特征1的权重。
* x1 指的是特征（已知输入项）。

要根据新的每分钟的鸣叫声值 x1**推断**温度y',只需要将 x1带入此模型即可。

下标（例如w1和 x1）预示着可以用多个特征来表示更复杂的模型。例如，具有三个特征的模型可以采用以下方程式：

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;y' = b + w1x1 + w2x2 + w3x3

### 训练与损失

简单来说，**训练**模型表示通过有标签样本来学习所有权重和偏差的理想值。在监督式学习中，机器学习算法通过以下方式构建模型：检查多个样本并尝试找出可最大限度减少损失的模型；这一过程成为**经验风险最小化**。

损失是对糟糕的预测的惩罚。也就是说，**损失**是一个数值，表示对于单个样本而言模型预测的准确程度。如果模型预测的完全准确，则损失为零，否则损失会较大。训练模型的目标是从所有样本中找到一组平均损失**较小**的权重和偏差。例如图3左侧显示的是损失较大的模型，右侧显示的是损失较小的模型。关于此图，注意一下几点：

* 红色箭头表示损失。
* 蓝色表示预测。

![4.png](./images/4.png)

**图 3. 左侧模型的损失较大；右侧模型的损失较小。**

请注意，左侧曲线图中的红色箭头比右侧曲线图中的对应红色箭头长的多。显然，相对于左侧曲线图中的蓝线，右侧曲线图中的蓝线代表的是预测效果更好的模型。

你可能想知道自己能否创建一个数学函数（损失函数），以有意义的方式汇总各个损失。

平方损失：一种常见的损失函数。

接下来我们要看的线性回归模型使用的是一种称为**平方损失**（又称为**L2损失**）的损失函数。单个样本的平方损失如下：

```
= the square of the difference between the label and the prediction
= (observation - prediction(x))²
= (y - y')²
```

**均方误差（MSE）**指的是每个样本的平均平方损失。要计算 MSE，请求出所有样本的平方损失之和，然后除以样本数量：

![](./images/5.png)

其中：

* （x , y）指的是样本，其中
  *  x 指的是模型进行预测时使用的特征集（例如温度、年龄和交配成功率）。
  *  y 指的是样本的标签（例如，每分钟的鸣叫次数）。
*  prediction(x)指的是权重和偏差与特征集 x 结合的函数。
*  D 指的是包含多个有标签样本（即(x, y)）的数据集。
*  N 指的是D 中的样本数量。

虽然 MSE 常用于机器学习，但它既不是唯一实用的损失函数，也不是适用于所有情形的最佳损失函数。

## 降低损失

为了训练模型，我们需要一种可降低模型损失的好方法。迭代方法是一种广泛用于降低损失的方法，而且使用起来简单有效。

### 降低损失：迭代方法

迭代学习可能会让您想到“Hot and Cold”这种寻找隐藏物品（如顶针）的儿童游戏。我们的游戏中，“隐藏的物品”就是最佳模型。刚开始，您会胡乱猜测（“w1的值为 0。”），等待系统告诉您损失是多少。然后，您再尝试另一种猜测（“w1 的值为 0.5。”），看看损失是多少。哎呀，这次更接近目标了。实际上，如果您以正确方式玩这个游戏，通常会越来越接近目标。这个游戏真正棘手的地方在于尽可能高效地找到最佳模型。

下图显示了机器学习算法用于训练模型的迭代试错过程：

![6.png](./images/6.png)

**图 1. 用于训练模型的迭代方法。**

我们将在整个机器学习过程中使用相同的迭代方法详细说明各种复杂情况，尤其是处于暴风雨中的蓝云区域。迭代策略在机器学习中的应用非常普遍，这主要是因为它可以很好的扩展到大型数据集。

“模型”部分将一个或多个特征作为输入，然后返回一个预测（y'）作为输出。为了进行简化，不妨考虑一种采用一个特征并返回预测的模型：

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;y' = b + w1x1

我们应该为 b 和 w1 设置那些初始值？对于线性回归问题，事实证明初始值并不重要。我们可以随机选值，不过我们还是选取以下这些无关紧要的值：

* b = 0
* w1 = 0

假设第一个特征值是10。将将该特征值带入预测函数会得到以下结果：

```
y' = 0 + 0 * 10
y' = 0
```

图中“计算损失”部分是模型将要使用的**损失函数**，假设我们使用平方损失函数。损失函数将采用两个输入值：

* y' :模型对特征 x 的预测值。
* y : 特征 x 对应的正确标签。

最后，我们来看图的“计算参数更新”部分。机器学习系统就是在此部分检查损失函数的值，并为 b 和 w1 生成新的值。现在，假设这个部分会产生新值，然后机器学习系统将根据所有标签重新评估所有特征，为损失函数生成一个新值，而该值又会产生新值。这种学习过程会持续迭代，直到该算法可能发现损失可能最低的模型参数。通常，你可以不断迭代，直到总体损失不再变化或至少变化极其缓慢为止。这时，我们可以说该模型已**收敛**。

_在训练机器学习模型时，首先对权重和偏差进行初始值猜测，然后反复调整这些猜测，直到获得损失可能最低的权重和偏差为止。_

### 降低损失：梯度下降法

迭代方法图含一个标题为“计算参数更新”的华而不实的绿框。现在，我们将用更实质的方法代替这种华而不实的算法。

假设我们有时间和计算资源来计算w1的所有可能值的损失。对于我们一直研究的回归问题，所产生的损失与w1的图形始终是凸形。换言之图形始终是碗状图，如下所示：

![7.png](./images/7.png)

**图 2. 回归问题产生的损失与权重图为凸形。**

凸形问题只有一个最低点；即只存在一个斜率正好为 0 的位置。这个最小值就是损失函数收敛之处。

通过计算整个数据集中w1每个可能值的损失函数来找到收敛点这种方法效率太低。我们来研究一种更好的机制，这种机制在机器学习领域非常热门，称为**梯度下降法**。

梯度下降法的第一个阶段是为 w1 选择一个起始值（起点）。起点并不重要；因此很多算法就直接将w1设为 0 或随机选择一个值。下图显示的是我们选择了一个稍大于 0 的起点：

![](./images/8.png)

**图 3. 梯度下降法的起点。**

然后，梯度下降法算法会计算损失曲线在起点处的梯度。简而言之，**梯度**是偏导数的矢量；它可以让您了解哪个方向距离目标“更近”或“更远”。请注意，损失相对于单个权重的梯度（如图 3 所示）就等于导数。

请注意，梯度是一个矢量，因此具有以下两个特征：

* 方向
* 大小

梯度始终指向损失函数中增长最为迅猛的方向。梯度下降法算法会沿着负梯度的方向走一步，以便尽快降低损失。

![](./images/9.png)

**图 4. 梯度下降法依赖于负梯度。**

为了确定损失函数曲线上的下一个点，梯度下降法算法会将梯度大小的一部分与起点相加，如下图所示：

![](./images/10.png)

**图 5. 一个梯度步长将我们移动到损失曲线上的下一个点。**

然后，梯度下降法会重复此过程，逐渐接近最低点。

### 降低损失:学习速率

正如之前所述，梯度矢量具有方向和大小。梯度下降法算法用梯度乘以一个称为**学习速率**（有时也成为**步长**）的标量，以确定下一个点的位置。例如，如果梯度大小为 2.5，学习速率为 0.01，则梯度下降法算法会选择距离前一个点 0.025 的位置作为下一个点。

**超参数**是编程人员在机器学习算法中用于调整的旋钮。大多数机器学习编程人员会花费相当多的时间来调整学习速率。如果您选择的学习速率过小，就会花费太长的学习时间：

![](./images/11.png)

**图 6. 学习速率过小。**

相反，如果您指定的学习速率过大，下一个点将永远在 U 形曲线的底部随意弹跳，就好像量子力学实验出现了严重错误一样：

![](./images/12.png)

**图 7. 学习速率过大。**

每个回归问题都存在一个金发姑娘学习速率。“金发姑娘”值与损失函数的平坦程度相关。如果您知道损失函数的梯度较小，则可以放心地试着采用更大的学习速率，以补偿较小的梯度并获得更大的步长。

![](./images/13.png)

**图 8. 学习速率恰恰好。**

详细了解理想的学习速率。

* 一维空间中的理想学习速率是 1/f''(x)（f(x) 对 x 的二阶导数的倒数）。
* 二维或多维空间中的理想学习速率是[海森矩阵](https://en.wikipedia.org/wiki/Hessian_matrix)（由二阶偏导数组成的矩阵）的倒数。
* 广义凸函数的情况则更为复杂。